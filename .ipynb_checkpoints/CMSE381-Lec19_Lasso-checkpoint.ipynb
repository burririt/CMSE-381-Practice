{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dfd68f5",
   "metadata": {},
   "source": [
    "# Lec 19 Lab: The Lasso\n",
    "## CMSE 381 - Fall 2023\n",
    "## Oct 18, 2023\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90aa0b0",
   "metadata": {},
   "source": [
    "In this module we are going to test out the lasso method, discussed in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea3a4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Everyone's favorite standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "\n",
    "\n",
    "# ML imports we've used previously\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f9a257",
   "metadata": {},
   "source": [
    "# Loading in the data\n",
    "\n",
    "Ok, here we go, let's play with a baseball data set again. Note this cleanup is all the same as the last lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceeb83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hitters_df = pd.read_csv('../../DataSets/Hitters.csv')\n",
    "\n",
    "# Print the dimensions of the original Hitters data (322 rows x 20 columns)\n",
    "print(\"Dimensions of original data:\", hitters_df.shape)\n",
    "\n",
    "# Drop any rows the contain missing values, along with the player names\n",
    "hitters_df = hitters_df.dropna().drop('Player', axis=1)\n",
    "\n",
    "# Replace any categorical variables with dummy variables\n",
    "hitters_df = pd.get_dummies(hitters_df, drop_first = True)\n",
    "\n",
    "hitters_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4672d6f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = hitters_df.Salary\n",
    "\n",
    "# Drop the column with the independent variable (Salary)\n",
    "X = hitters_df.drop(['Salary'], axis = 1).astype('float64')\n",
    "\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76813de",
   "metadata": {},
   "source": [
    "Finally, here's a list of $\\alpha$s to test for our Lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbf296e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of alphas\n",
    "alphas = 10**np.linspace(4,-2,100)*0.5\n",
    "alphas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b69746",
   "metadata": {},
   "source": [
    "# Lasso \n",
    "\n",
    "Thanks to the wonders of `scikit-learn`, now that we know how to do all this with ridge regression, translation to lasso is super easy. \n",
    "\n",
    "- [Lasso Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)\n",
    "- [LassoCV Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html#sklearn.linear_model.LassoCV)\n",
    "- [User guide](https://scikit-learn.org/stable/modules/linear_model.html#lasso)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe822edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, LassoCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e181b9",
   "metadata": {},
   "source": [
    "Here's an example computing the lasso regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c0df01",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1 #<------ this is me picking an alpha value\n",
    "\n",
    "# normalize the input\n",
    "transformer = StandardScaler().fit(X)\n",
    "# transformer.set_output(transform = 'pandas') #<---- some older versions of sklearn\n",
    "                                               #      have issues with this\n",
    "X_norm = pd.DataFrame(transformer.transform(X), columns = X.columns)\n",
    "\n",
    "# Fit the regression\n",
    "lasso = Lasso(alpha = a) \n",
    "lasso.fit(X_norm, y)\n",
    "\n",
    "# Get all the coefficients\n",
    "print('intercept:', lasso.intercept_)\n",
    "print('\\n')\n",
    "print(pd.Series(lasso.coef_, index = X_norm.columns))\n",
    "print('\\nTraining MSE:',mean_squared_error(y,lasso.predict(X_norm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b06beb",
   "metadata": {},
   "source": [
    "And the version using the `make_pipeline` along with the `StandardScaler` function which gives back the same answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d948bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1#<------ this is me picking an alpha value\n",
    "\n",
    "\n",
    "# The make_pipeline command takes care of the normalization and the \n",
    "# Lasso regression for you. Note that I am passing in the un-normalized\n",
    "# matrix X everywhere in here, since the normalization happens internally.\n",
    "model = make_pipeline(StandardScaler(), Lasso(alpha = a))\n",
    "model.fit(X, y)\n",
    " \n",
    "# Get all the coefficients. Notice that in order to get \n",
    "# them out of the ridge portion, we have to ask the pipeline \n",
    "# for the specific bit we want with the model.named_steps['ridge']\n",
    "# in place of just ridge from above.\n",
    "print('intercept:', model.named_steps['lasso'].intercept_)\n",
    "print('\\n')\n",
    "print(pd.Series(model.named_steps['lasso'].coef_, index = X.columns))\n",
    "print('\\nTraining MSE:',mean_squared_error(y,model.predict(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0c2da4",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>Do this:</font>** Mess with the values for $a$ in the code, such as for $a=1, 10, 100, 1000$. What do you notice about the coefficients? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af63a093",
   "metadata": {},
   "source": [
    "*Your answer here*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4929bd",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>Do this:</font>** Make a graph of the coeffiencts from lasso as $\\alpha$ changes \n",
    "\n",
    "*Note: we did similar things in the last class, I've included the code you can borrow and modify from there. Also note I got a bunch of convergence warnings, but drawing the graphs I could safely ignore them.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1463365d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for computing the coefficients goes here \n",
    "# I've included the code from last class that did this for the ridge version\n",
    "# so you should just need to update it to do lasso instead.\n",
    "\n",
    "\n",
    "coefs = []\n",
    "\n",
    "for a in alphas:\n",
    "    model = make_pipeline(StandardScaler(), Ridge(alpha = a))\n",
    "    model.fit(X, y)\n",
    "    coefs.append(model.named_steps['ridge'].coef_)\n",
    "    \n",
    "\n",
    "coefs = pd.DataFrame(coefs,columns = X_norm.columns)\n",
    "coefs.head()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900f7150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If that worked above, you'll get a graph in this code. \n",
    "\n",
    "for var in coefs.columns:\n",
    "    # I'm greying out the ones that have magnitude below 200 to easier visualization\n",
    "    if np.abs(coefs[var][coefs.shape[0]-1])<200:\n",
    "        plt.plot(alphas, coefs[var], color = 'grey', linewidth = .5)\n",
    "    else:\n",
    "        plt.plot(alphas, coefs[var], label = var)\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('coefficients')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e656dec1",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>Do this:</font>** Make a graph the test mean squared error as $\\alpha$ changes for a fixed train/test split. \n",
    "\n",
    "*Note: again I've included code from last time that you should just have to update.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6b2936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update this code to get the MSE for lasso instead of Ridge\n",
    "X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "errors = []\n",
    "\n",
    "for a in alphas:\n",
    "    model = make_pipeline(StandardScaler(), Ridge(alpha = a))\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    pred = model.predict(X_test)\n",
    "    errors.append(mean_squared_error(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513ec63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If that worked, you should see your test error here.\n",
    "i = np.argmin(errors) # Index of minimum \n",
    "print('Min occurs at alpha = ', alphas[i])\n",
    "print('Min MSE is', errors[i])\n",
    "\n",
    "plt.title('Testing MSE')\n",
    "plt.plot(alphas,errors)\n",
    "plt.scatter(alphas[i],errors[i])\n",
    "ax=plt.gca()\n",
    "ax.set_xscale('log')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0238b939",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>Do this:</font>** make a plot showing the number of non-zero coefficients in each model.\n",
    "\n",
    "*Hint: I used the `np.count_nonzero` command*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2b61b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e8c64c",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>Q:</font>** Say your goal was to end up with a model with 5 variables used. What choice of $\\alpha$ gives us that and what are the variables used?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bd49f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372b7ab6",
   "metadata": {},
   "source": [
    "## Lasso with Cross Validation\n",
    "\n",
    "&#9989; **<font color=red>Do this:</font>** Now try what we did with `LassoCV`.  What choice of $\\alpha$ does it recommend? \n",
    "\n",
    "*I would actually recommend either not passing in any $\\alpha$ list or passing explicitly `alphas = None`. `RidgeCV` can't do this, but `LassoCV` will automatically try to find good choices of $\\alpha$ for you.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4b5fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's the ridge code from last time that you should update\n",
    "\n",
    "X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "# To make sure my normalization isn't snooping, I fit the transformer only \n",
    "# on the training set \n",
    "transformer = StandardScaler().fit(X_train)\n",
    "# transformer.set_output(transform = 'pandas')\n",
    "X_train_norm = pd.DataFrame(transformer.transform(X_train), columns = X_train.columns)\n",
    "\n",
    "# but in order for my output results to make sense, I have to apply the same \n",
    "# transformation to the testing set. \n",
    "X_test_norm = transformer.transform(X_test)\n",
    "\n",
    "# I'm going to drop that 0 from the alphas because it makes \n",
    "# RidgeCV cranky\n",
    "alphas = alphas[:-1]\n",
    "\n",
    "\n",
    "ridgecv = RidgeCV(alphas = alphas, \n",
    "                  scoring = 'neg_mean_squared_error', \n",
    "                  )\n",
    "ridgecv.fit(X_train_norm, y_train)\n",
    "\n",
    "print('alpha chosen is', ridgecv.alpha_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe50336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d3ee5d",
   "metadata": {},
   "source": [
    "Now let's take a look at some of the coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691eabf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Some of the coefficients are now reduced to exactly zero.\n",
    "pd.Series(lassocv.coef_, index=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56b02f1",
   "metadata": {},
   "source": [
    "&#9989; **<font color=red>Q:</font>** We've been repeating over and over that lasso gives us coefficients that are actually 0.  At least in my code, I'm not seeing many that are 0. What happened? Can I change something to get more 0 entries? \n",
    "\n",
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da8e0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You might also want some code in here to try to figure it out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f79113",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "-----\n",
    "### Congratulations, we're done!\n",
    "Written by Dr. Liz Munch, Michigan State University\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\">Creative Commons Attribution-NonCommercial 4.0 International License</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb8354f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
